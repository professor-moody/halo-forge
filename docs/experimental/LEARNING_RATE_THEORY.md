# Learning Rate Strategies for RLVR Training

> **⚠️ EXPERIMENTAL & THEORETICAL**
> 
> This document represents our current understanding and hypotheses about learning rate optimization for RLVR/RAFT training. These recommendations have NOT been empirically validated in production runs. Use as a starting point for experimentation, not as proven best practices.
>
> Last updated: January 2, 2026

---

## Real-World Observations (What Prompted This Research)

During production RAFT training on **AMD Strix Halo** (128GB unified memory, gfx1151):

| Cycle | Compile Rate | LR Used | Observation |
|-------|--------------|---------|-------------|
| 1 | Baseline | 5e-5 | Learning established |
| 2-5 | Improving | 5e-5 | Consistent gains |
| 6 | Peak | 5e-5 | Best performance |
| 7 | Declining | 5e-5 | Degradation begins |
| 8 | Declining | 5e-5 | Further degradation |

**Key finding**: Using **constant learning rate (5e-5)** across all cycles led to degradation after cycle 6.

This motivated the hypothesis: **decreasing LR across cycles might prevent late-cycle degradation**.

---

## Table of Contents

1. [Conceptual Foundation](#conceptual-foundation)
2. [Why RLVR is Different](#why-rlvr-is-different)
3. [Learning Rate Selection](#learning-rate-selection)
4. [Cross-Cycle Decay Strategies](#cross-cycle-decay-strategies)
5. [Within-Cycle Schedules](#within-cycle-schedules)
6. [Interaction Effects](#interaction-effects)
7. [Diagnostic Signals](#diagnostic-signals)
8. [Recommended Experiments](#recommended-experiments)
9. [Configuration Templates](#configuration-templates)
10. [Open Questions](#open-questions)

---

## Conceptual Foundation

### What Learning Rate Controls

Learning rate (LR) determines the step size when updating model parameters:

```
new_weights = old_weights - learning_rate × gradient
```

**Too high**: Overshoot optima, oscillate, diverge, catastrophic forgetting
**Too low**: Slow convergence, stuck in local minima, wasted compute

### The Fine-Tuning Context

Unlike pre-training (random initialization), fine-tuning starts from a capable model:

```
Pre-training:    Random → Everything to learn    → Higher LR tolerable (1e-4 to 1e-3)
SFT:             Capable → Adapt to domain       → Moderate LR (5e-5 to 3e-4)
RAFT:            Already tuned → Refine behavior → Lower LR (1e-5 to 5e-5)
```

**Key insight**: Each stage starts from a better initialization, requiring more careful updates.

### LoRA's Impact on Learning Rate

With full fine-tuning, you'd use very conservative LR to avoid destroying pre-trained knowledge. LoRA changes this:

| Aspect | Full Fine-Tuning | LoRA |
|--------|------------------|------|
| Parameters updated | 100% | ~0.1-1% |
| Risk of forgetting | High | Low |
| Typical LR | 1e-5 to 5e-5 | 1e-4 to 3e-4 |
| Convergence speed | Slow | Fast |

**LoRA safety margin**: Since you're only updating adapter weights, higher LR is safe—the base model remains frozen.

---

## Why RLVR is Different

### Standard SFT vs. RAFT

| Aspect | SFT | RAFT |
|--------|-----|------|
| Data source | Fixed, curated dataset | Model's own outputs (filtered) |
| Data quality | Consistent | Variable (verified but model-generated) |
| Data distribution | Static | Shifts each cycle |
| Primary risk | Overfit to training data | Collapse to narrow distribution |
| Goal | Learn new capabilities | Reinforce successful behaviors |

### The Distribution Shift Problem

In RAFT, each cycle trains on data generated by the previous cycle's model:

```
Cycle 1: Train on filtered outputs from base model
Cycle 2: Train on filtered outputs from Cycle 1 model
Cycle 3: Train on filtered outputs from Cycle 2 model
...
```

This creates **compounding effects**:
- Good: Model improves at generating verified code
- Bad: Distribution narrows, diversity decreases
- Risk: Mode collapse to single "safe" pattern

**Hypothesis**: Decreasing LR across cycles counteracts narrowing by making smaller updates as the distribution contracts.

### The Collapse Mechanism (Theoretical)

```
High LR + Narrowing Data → Aggressive updates toward limited patterns
                        → Reduced exploration
                        → Even narrower next-cycle data
                        → Feedback loop → Collapse
```

**Counter-hypothesis**: Low LR might preserve diversity but slow learning. The optimal trajectory is unknown.

---

## Learning Rate Selection

### Starting Point Recommendations

> **⚠️ THEORETICAL**: Based on literature and reasoning, not empirical validation.

| Phase | Recommended LR | Confidence | Rationale |
|-------|---------------|------------|-----------|
| SFT (LoRA) | 1e-4 to 3e-4 | High | Well-established in literature |
| RAFT Cycle 1 | 5e-5 | Medium | ~1/4 of SFT LR |
| RAFT Cycle 3 | 3e-5 | Low | Theoretical decay |
| RAFT Cycle 5 | 2e-5 | Low | Theoretical decay |
| RAFT Cycle 7+ | 1e-5 | Speculative | Minimal updates |

### The 1/4 Rule (Hypothesis)

**Proposed heuristic**: Start RAFT at 1/4 of your SFT learning rate.

```
SFT LR: 2e-4
RAFT Cycle 1 LR: 5e-5  (2e-4 / 4)
```

**Rationale**: RAFT data is noisier (model-generated) and goal is refinement not learning. More conservative updates appropriate.

**Status**: Untested hypothesis. May need adjustment based on:
- Model size
- Dataset characteristics
- Verification signal quality

---

## Cross-Cycle Decay Strategies

### Strategy A: Manual Step Decay

Explicitly set LR for each cycle:

```yaml
learning_rate_schedule:
  cycle_1: 5e-5
  cycle_2: 4e-5
  cycle_3: 3e-5
  cycle_4: 2.5e-5
  cycle_5: 2e-5
```

**Pros**: Full control, can adjust based on observed behavior
**Cons**: Requires manual tuning, no principled basis

### Strategy B: Exponential Decay

```python
def get_cycle_lr(base_lr: float, cycle: int, decay: float = 0.85) -> float:
    """
    Exponential decay across RAFT cycles.
    
    THEORETICAL - not empirically validated.
    
    Args:
        base_lr: Starting learning rate (cycle 1)
        cycle: Current cycle number (1-indexed)
        decay: Decay factor per cycle (0.8-0.95 reasonable range)
    
    Returns:
        Learning rate for this cycle
    """
    return base_lr * (decay ** (cycle - 1))
```

Example trajectory with `base_lr=5e-5, decay=0.85`:

| Cycle | Learning Rate |
|-------|---------------|
| 1 | 5.00e-5 |
| 2 | 4.25e-5 |
| 3 | 3.61e-5 |
| 4 | 3.07e-5 |
| 5 | 2.61e-5 |
| 6 | 2.22e-5 |

### Strategy C: Performance-Based Decay

Reduce LR when improvement stalls:

```python
def adaptive_lr(
    base_lr: float,
    cycle: int,
    prev_metric: float,
    curr_metric: float,
    improvement_threshold: float = 0.02,
    decay_factor: float = 0.7
) -> float:
    """
    Reduce LR if improvement is below threshold.
    
    HIGHLY SPECULATIVE - complex dynamics not understood.
    """
    if cycle == 1:
        return base_lr
    
    improvement = (curr_metric - prev_metric) / prev_metric
    
    if improvement < improvement_threshold:
        return base_lr * decay_factor
    else:
        return base_lr
```

**Warning**: This creates complex feedback dynamics. May cause instability.

### Strategy D: Warmup-Stable-Decay Across Cycles

```
Cycles 1-2:  Warmup phase    → Higher LR (5e-5), establish learning
Cycles 3-4:  Stable phase    → Moderate LR (3e-5), consistent improvement
Cycles 5+:   Refinement      → Lower LR (1.5e-5), polish without disruption
```

**Analogy**: Similar to pre-training schedules but across RAFT cycles instead of steps.

---

## Within-Cycle Schedules

Each RAFT cycle involves a short training run (typically 1 epoch). The within-cycle schedule still matters.

### Warmup

**Purpose**: Prevent early instability from aggressive updates on first batches.

```yaml
# For short training (1 epoch, ~500 samples, batch 32)
# Total steps ≈ 500 / 32 = ~16 steps

warmup_steps: 5  # ~30% of training

# For longer training, use ratio
warmup_ratio: 0.1  # 10% warmup
```

**RAFT-specific consideration**: With small filtered datasets, you might have very few steps. Warmup of 5-10 steps is reasonable.

### Scheduler Types

| Scheduler | Shape | Best For | RAFT Suitability |
|-----------|-------|----------|------------------|
| `constant` | Flat | Very short training | Good |
| `linear` | Linear decay to 0 | Standard choice | Good |
| `cosine` | Smooth S-curve | Longer training | Overkill for 1 epoch |
| `cosine_with_restarts` | Oscillating | Multi-phase training | Not recommended |

**Recommendation for RAFT**: `linear` or `constant`. With 1 epoch of training, sophisticated schedules provide minimal benefit.

### The Effective Steps Problem

With gradient accumulation:
```
actual_steps = (num_samples / batch_size) / gradient_accumulation_steps
```

Example:
```
500 samples / 2 batch / 16 accumulation = ~16 optimizer steps
```

With only 16 steps, warmup and scheduler effects are minimal. **Focus on getting the base LR right.**

---

## Interaction Effects

### LR × Batch Size

The **linear scaling rule** (approximate):
```
If batch_size doubles, LR can double
```

Your configuration:
```yaml
batch_size: 2
gradient_accumulation_steps: 16
effective_batch_size: 32
```

If you changed to `effective_batch_size: 64`, theory suggests doubling LR. But:

**Caution for RAFT**: Linear scaling was derived for pre-training. With RAFT's distribution shift, more conservative scaling may be appropriate.

### LR × LoRA Rank (r)

Higher rank = more parameters = potentially different LR needs.

| LoRA r | Parameters | Suggested LR Adjustment |
|--------|------------|------------------------|
| 8 | Fewer | May tolerate higher LR |
| 16 | Moderate | Baseline |
| 32 | More | May need lower LR |
| 64 | Many | Definitely lower LR |

**Note**: This interaction is not well-studied for RAFT specifically.

### LR × Alpha/r Ratio

LoRA's effective scaling:
```
effective_scale = alpha / r
```

Your config: `alpha=32, r=16 → scale=2.0`

This means LoRA updates are multiplied by 2. Some interpret this as:
```
nominal_lr = 2e-4
effective_lr ≈ 4e-4 (due to 2x scaling)
```

**If you change alpha/r ratio, reconsider LR.**

### LR × Temperature (Generation)

Higher generation temperature → more diverse samples → potentially allows higher training LR (more varied training signal).

Lower temperature → less diverse samples → lower training LR advisable (narrower signal).

**Hypothesis**: Temperature and LR should be considered together. High temp + high LR or low temp + low LR may be stable configurations.

---

## Diagnostic Signals

### Signs Learning Rate is Too High

| Signal | How to Detect |
|--------|---------------|
| Loss oscillation | Training loss jumps up and down |
| Divergence | Loss increases consistently |
| Validation gap | Train loss ↓ but val loss ↑ |
| Quality degradation | Cycle N+1 worse than Cycle N |
| Repetitive outputs | Model generates same pattern repeatedly |
| Gradient clipping | Gradients frequently hit `max_grad_norm` |

### Signs Learning Rate is Too Low

| Signal | How to Detect |
|--------|---------------|
| Flat loss | Training loss barely changes |
| Slow improvement | Many cycles with minimal metric gain |
| Wasted compute | Hours of training, no improvement |

### Monitoring Recommendations

```python
# Key metrics to track per cycle
metrics_to_log = {
    "train_loss": "Should decrease within cycle",
    "gradient_norm": "Watch for frequent clipping",
    "verification_rate": "Primary success metric",
    "output_diversity": "Entropy of generated samples",
    "cycle_improvement": "Delta from previous cycle",
}
```

### The Gradient Norm Signal

Your config has `max_grad_norm: 0.3`. This clips large gradients for stability.

```
If gradient_norm frequently = 0.3:  → LR probably too high
If gradient_norm typically = 0.1-0.2: → Healthy range
If gradient_norm consistently < 0.05: → LR might be too low
```

---

## Recommended Experiments

> **⚠️ These are proposed experiments, not validated approaches.**

### Experiment 1: Baseline Characterization

**Goal**: Establish baseline behavior before optimizing.

```yaml
# Run 5 cycles with constant LR
learning_rate: 5e-5  # constant across all cycles
cycles: 5
```

**Measure**: Verification rate per cycle. Does it plateau? Degrade?

### Experiment 2: Decay Factor Sweep

**Goal**: Find optimal decay rate.

```yaml
# Run A: Aggressive decay
decay_factor: 0.7

# Run B: Moderate decay  
decay_factor: 0.85

# Run C: Conservative decay
decay_factor: 0.95
```

**Measure**: Which achieves highest peak performance? Latest degradation?

### Experiment 3: Starting LR Sweep

**Goal**: Find optimal cycle-1 learning rate.

```yaml
# Run A
base_lr: 3e-5

# Run B
base_lr: 5e-5

# Run C
base_lr: 8e-5
```

**Measure**: Cycle 1 improvement, stability across cycles.

### Experiment 4: Interaction with Temperature

**Goal**: Understand LR-temperature interaction.

| Config | Temperature | LR |
|--------|-------------|-----|
| A | 0.5 | 3e-5 |
| B | 0.7 | 5e-5 |
| C | 0.9 | 7e-5 |

**Hypothesis**: Higher temperature allows higher LR.

---

## Configuration Templates

See accompanying files:
- `configs/experimental/raft_constant_lr.yaml` - Baseline constant LR
- `configs/experimental/raft_decay_lr.yaml` - Exponential decay
- `configs/experimental/raft_aggressive_decay.yaml` - Faster decay for short runs

---

## Open Questions

### Unanswered (Require Experimentation)

1. **What's the optimal decay factor?** 0.85 is a guess. Could be 0.7 or 0.95.

2. **Does dataset size affect optimal LR?** Larger filtered sets might tolerate higher LR.

3. **Is per-cycle decay necessary?** Maybe constant LR works fine for 5 cycles.

4. **How does verification signal quality affect LR?** Binary (compile/fail) vs. graduated rewards.

5. **Should early cycles have higher LR?** Current assumption, not validated.

### Theoretical Uncertainties

1. **Distribution shift dynamics**: How exactly does the training distribution narrow? Is it smooth or sudden?

2. **Optimal stopping**: How to detect optimal cycle to stop before degradation?

3. **Recovery from degradation**: If cycle N degrades, can lower LR in cycle N+1 recover?

4. **Transfer to other domains**: Do these principles apply to non-code RLVR?

---

## References

### Relevant Literature

- **LoRA**: Hu et al., 2021 - Low-Rank Adaptation of Large Language Models
- **RAFT**: Dong et al., 2023 - Reward rAnked FineTuning
- **Linear Scaling**: Goyal et al., 2017 - Accurate, Large Minibatch SGD

### Related Work in halo-forge

- `docs/THEORY.md` - General RLVR theory
- `docs/HARDWARE_NOTES.md` - Strix Halo optimization notes
- `configs/sft_example.yaml` - SFT configuration reference

### Hardware Context

All research conducted on **AMD Strix Halo**:
- **Memory**: 128GB unified LPDDR5X (CPU + GPU share same pool)
- **Architecture**: gfx1151
- **Key characteristic**: Compute-bound (96-99% GPU utilization with BF16)
- **Implication**: Memory is not the bottleneck; LR dynamics may differ from memory-constrained setups

---

## Changelog

| Date | Change |
|------|--------|
| 2026-01-02 | Initial document, pre-production run |

---

> **Reminder**: This document is theoretical. Update with empirical findings as experiments complete.
