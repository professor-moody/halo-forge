"""
RLVR Python Verification

Combines generated code with test cases from dataset for verification.
Designed for HumanEval and MBPP RLVR training.

Reward System:
- 1.0: All tests pass
- 0.0: Any test fails (binary pass/fail)

Note: Unlike C++ verifiers which have graduated rewards (compile/run/correct),
Python verification is binary because test assertions either pass or fail.

Usage:
    verifier = HumanEvalVerifier("data/rlvr/humaneval_full.jsonl")
    result = verifier.verify(generated_code, task_id="HumanEval/0")
"""

import json
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, Optional, List, Any
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed


@dataclass
class VerifyResult:
    """Result of code verification."""
    success: bool
    reward: float
    details: str
    error: Optional[str] = None
    tests_passed: int = 0
    tests_total: int = 0


class RLVRPytestVerifier:
    """
    Python verifier for RLVR training (HumanEval/MBPP).
    
    Loads test cases from dataset JSONL files and combines with generated
    code for verification. Returns binary rewards (1.0 pass, 0.0 fail).
    
    Note: Despite the name, this runs tests with `python` directly, not pytest.
    The name is kept for backwards compatibility.
    
    Example:
        verifier = RLVRPytestVerifier("data/rlvr/humaneval_full.jsonl")
        
        # During RAFT generation
        generated_code = model.generate(prompt)
        result = verifier.verify(generated_code, task_id="HumanEval/0")
        
        if result.success:  # reward = 1.0
            # Add to training set
    """
    
    def __init__(
        self,
        dataset_path: str,
        timeout: int = 30,
        max_workers: int = 8
    ):
        """
        Initialize verifier with dataset.
        
        Args:
            dataset_path: Path to JSONL file with tests (must contain task_id, prompt, tests)
            timeout: Test execution timeout in seconds
            max_workers: Max parallel workers for verification
        """
        self.timeout = timeout
        self.max_workers = max_workers
        self.test_cases: Dict[str, Dict] = {}
        self.prompt_to_task_id: Dict[str, str] = {}  # Reverse lookup
        self.dataset_type = "humaneval"  # Default, override in subclasses
        
        self._load_dataset(dataset_path)
    
    def _load_dataset(self, path: str) -> None:
        """Load test cases from JSONL file."""
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"Dataset not found: {path}")
        
        with open(path) as f:
            for line in f:
                record = json.loads(line)
                task_id = str(record["task_id"])
                self.test_cases[task_id] = record
                
                # Build reverse lookup: prompt text -> task_id
                prompt = record.get("prompt", "")
                if prompt:
                    # Store first 200 chars as key (enough to be unique)
                    prompt_key = prompt[:200].strip()
                    self.prompt_to_task_id[prompt_key] = task_id
        
        print(f"Loaded {len(self.test_cases)} test cases from {path}")
    
    def get_test_code(self, task_id: str) -> Optional[str]:
        """Get test code for a task."""
        if task_id not in self.test_cases:
            return None
        return self.test_cases[task_id].get("tests", "")
    
    def get_entry_point(self, task_id: str) -> str:
        """Get entry point function name for a task."""
        if task_id not in self.test_cases:
            return "solution"
        return self.test_cases[task_id].get("entry_point", "solution")
    
    def extract_code(self, response: str) -> str:
        """Extract code from model response (handles markdown blocks)."""
        import re
        
        # Try to find code in markdown blocks
        patterns = [
            r'```python\n(.*?)```',
            r'```\n(.*?)```',
            r'```(.*?)```'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # No markdown block, return as-is
        return response.strip()
    
    def build_test_file(
        self,
        generated_code: str,
        task_id: str,
        dataset_type: str = "humaneval"
    ) -> str:
        """
        Build complete test file combining code and tests.
        
        Args:
            generated_code: Code generated by model
            task_id: Task identifier
            dataset_type: "humaneval" or "mbpp"
            
        Returns:
            Complete Python code ready for pytest
        """
        code = self.extract_code(generated_code)
        test_code = self.get_test_code(task_id)
        entry_point = self.get_entry_point(task_id)
        
        if not test_code:
            raise ValueError(f"No test cases found for task: {task_id}")
        
        if dataset_type == "humaneval":
            # HumanEval format:
            # - prompt includes function signature
            # - test has check(candidate) function
            # - Need to call check(entry_point)
            return self._build_humaneval_test(code, test_code, entry_point)
        else:
            # MBPP format:
            # - tests are assertions that call the function directly
            return self._build_mbpp_test(code, test_code)
    
    def _build_humaneval_test(
        self,
        code: str,
        test_code: str,
        entry_point: str
    ) -> str:
        """Build test file for HumanEval format."""
        # HumanEval tests look like:
        #   def check(candidate):
        #       assert candidate([1,2,3]) == True
        #   check(has_close_elements)
        
        return f'''# Generated solution
{code}

# Test code
{test_code}

# Run check
check({entry_point})
'''
    
    def _build_mbpp_test(self, code: str, test_code: str) -> str:
        """Build test file for MBPP format."""
        # MBPP tests use test_solution() function
        return f'''# Generated solution
{code}

# Tests
{test_code}

# Run tests
test_solution()
'''
    
    def verify(
        self,
        generated_code: str,
        task_id: str,
        dataset_type: str = "humaneval"
    ) -> VerifyResult:
        """
        Verify generated code against test cases.
        
        Combines generated code with test assertions from dataset,
        writes to temp file, and executes with Python.
        
        Rewards are binary:
        - 1.0: All tests pass (return code 0)
        - 0.0: Any test fails or execution error
        
        Args:
            generated_code: Code generated by model
            task_id: Task identifier to look up tests
            dataset_type: "humaneval" or "mbpp"
            
        Returns:
            VerifyResult with success/failure and reward
        """
        try:
            # Build complete test file
            test_file_content = self.build_test_file(
                generated_code, task_id, dataset_type
            )
        except ValueError as e:
            return VerifyResult(
                success=False,
                reward=0.0,
                details=str(e),
                error=str(e)
            )
        
        # Run in temp directory
        with tempfile.TemporaryDirectory() as tmpdir:
            test_file = Path(tmpdir) / "test_solution.py"
            test_file.write_text(test_file_content)
            
            # HumanEval uses check(candidate) - run directly with python
            # MBPP uses assertions - also works with direct python execution
            cmd = ["python", str(test_file)]
            
            try:
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=self.timeout,
                    cwd=tmpdir
                )
                
                success = result.returncode == 0
                
                # Binary rewards: all tests pass (1.0) or any fail (0.0)
                # Python test assertions are all-or-nothing per execution
                reward = 1.0 if success else 0.0
                tests_passed = 1 if success else 0
                tests_total = 1
                
                return VerifyResult(
                    success=success,
                    reward=reward,
                    details=result.stdout[-500:] if result.stdout else "",
                    error=result.stderr[-500:] if not success else None,
                    tests_passed=tests_passed,
                    tests_total=tests_total
                )
                
            except subprocess.TimeoutExpired:
                return VerifyResult(
                    success=False,
                    reward=0.0,
                    details="Test execution timed out",
                    error=f"Timeout after {self.timeout}s"
                )
            except Exception as e:
                return VerifyResult(
                    success=False,
                    reward=0.0,
                    details="Test execution failed",
                    error=str(e)
                )
    
    def _parse_pytest_output(self, output: str) -> tuple:
        """Parse pytest output for pass/fail counts."""
        import re
        
        # Look for "X passed" or "X passed, Y failed"
        match = re.search(r'(\d+) passed', output)
        passed = int(match.group(1)) if match else 0
        
        match = re.search(r'(\d+) failed', output)
        failed = int(match.group(1)) if match else 0
        
        match = re.search(r'(\d+) error', output)
        errors = int(match.group(1)) if match else 0
        
        total = passed + failed + errors
        if total == 0:
            total = 1  # Avoid division by zero
        
        return passed, total
    
    def verify_batch(
        self,
        codes: List[str],
        prompts: Optional[List[str]] = None
    ) -> List[VerifyResult]:
        """
        Verify multiple samples in parallel.
        
        Args:
            codes: List of generated code strings
            prompts: List of prompts (used to look up task_id)
            
        Returns:
            List of VerifyResult
        """
        if not prompts:
            # Can't verify without prompts
            return [
                VerifyResult(
                    success=False,
                    reward=0.0,
                    details="No prompt provided for task_id lookup",
                    error="RLVRPytestVerifier requires prompts for verification"
                )
                for _ in codes
            ]
        
        # Parallel verification using ThreadPoolExecutor
        results = [None] * len(codes)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._verify_with_prompt, code, prompt): i
                for i, (code, prompt) in enumerate(zip(codes, prompts))
            }
            
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    results[idx] = future.result()
                except Exception as e:
                    results[idx] = VerifyResult(
                        success=False,
                        reward=0.0,
                        details="Verification exception",
                        error=str(e)
                    )
        
        return results
    
    def _verify_with_prompt(self, code: str, prompt: str) -> VerifyResult:
        """Verify code using prompt to look up task_id."""
        # Look up task_id from prompt
        prompt_key = prompt[:200].strip()
        task_id = self.prompt_to_task_id.get(prompt_key)
        
        if not task_id:
            return VerifyResult(
                success=False,
                reward=0.0,
                details=f"Could not find task_id for prompt: {prompt_key[:50]}...",
                error="Unknown prompt"
            )
        
        # Call parent class verify with all 3 args (subclasses override with 2 args)
        return RLVRPytestVerifier.verify(self, code, task_id, self.dataset_type)
    
    def cleanup(self):
        """Cleanup resources. No-op for this verifier."""
        pass


class HumanEvalVerifier(RLVRPytestVerifier):
    """Convenience class for HumanEval verification."""
    
    def __init__(self, dataset_path: str = "data/rlvr/humaneval_full.jsonl", **kwargs):
        super().__init__(dataset_path, **kwargs)
        self.dataset_type = "humaneval"
    
    def verify(self, generated_code: str, task_id: str) -> VerifyResult:
        return super().verify(generated_code, task_id, self.dataset_type)


class MBPPVerifier(RLVRPytestVerifier):
    """Convenience class for MBPP verification."""
    
    def __init__(self, dataset_path: str = "data/rlvr/mbpp_train_full.jsonl", **kwargs):
        super().__init__(dataset_path, **kwargs)
        self.dataset_type = "mbpp"
    
    def verify(self, generated_code: str, task_id: str) -> VerifyResult:
        return super().verify(generated_code, task_id, self.dataset_type)


# =============================================================================
# Quick test
# =============================================================================

if __name__ == "__main__":
    import sys
    
    # Test with MBPP-style format (test_solution function)
    test_dataset = [
        {
            "task_id": "test/0",
            "prompt": "Write a function add(a, b) that returns a + b",
            "tests": "def test_solution():\n    assert add(1, 2) == 3\n    assert add(0, 0) == 0",
            "entry_point": "add"
        }
    ]
    
    # Write temp dataset
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        for record in test_dataset:
            f.write(json.dumps(record) + '\n')
        temp_path = f.name
    
    try:
        verifier = RLVRPytestVerifier(temp_path)
        
        # Test correct code
        correct_code = "def add(a, b):\n    return a + b"
        result = verifier.verify(correct_code, "test/0", "mbpp")
        print(f"Correct code: success={result.success}, reward={result.reward}")
        assert result.success, "Correct code should pass"
        
        # Test incorrect code
        wrong_code = "def add(a, b):\n    return a - b"
        result = verifier.verify(wrong_code, "test/0", "mbpp")
        print(f"Wrong code: success={result.success}, reward={result.reward}")
        assert not result.success, "Wrong code should fail"
        
        # Test code that doesn't compile
        broken_code = "def add(a, b)\n    return a + b"
        result = verifier.verify(broken_code, "test/0", "mbpp")
        print(f"Broken code: success={result.success}, reward={result.reward}")
        assert not result.success, "Broken code should fail"
        
        print("\nAll self-tests passed!")
        
    finally:
        Path(temp_path).unlink()
