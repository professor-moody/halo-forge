"""
RLVR Pytest Verification Wrapper

Combines generated code with test cases from dataset for verification.
Designed for HumanEval and MBPP RLVR training.

Usage:
    verifier = RLVRPytestVerifier("data/rlvr/humaneval_full.jsonl")
    result = verifier.verify(generated_code, task_id="HumanEval/0")
"""

import json
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, Optional, List, Any
from dataclasses import dataclass


@dataclass
class VerifyResult:
    """Result of code verification."""
    success: bool
    reward: float
    details: str
    error: Optional[str] = None
    tests_passed: int = 0
    tests_total: int = 0


class RLVRPytestVerifier:
    """
    Pytest verifier for RLVR training.
    
    Loads test cases from dataset and combines with generated code
    for verification.
    
    Example:
        verifier = RLVRPytestVerifier("data/rlvr/humaneval_full.jsonl")
        
        # During RAFT generation
        generated_code = model.generate(prompt)
        result = verifier.verify(generated_code, task_id="HumanEval/0")
        
        if result.success:
            # Add to training set
    """
    
    def __init__(
        self,
        dataset_path: str,
        timeout: int = 30,
        reward_partial: bool = True
    ):
        """
        Initialize verifier with dataset.
        
        Args:
            dataset_path: Path to JSONL file with tests
            timeout: Test execution timeout in seconds
            reward_partial: Give partial reward for some tests passing
        """
        self.timeout = timeout
        self.reward_partial = reward_partial
        self.test_cases: Dict[str, Dict] = {}
        
        self._load_dataset(dataset_path)
    
    def _load_dataset(self, path: str) -> None:
        """Load test cases from JSONL file."""
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"Dataset not found: {path}")
        
        with open(path) as f:
            for line in f:
                record = json.loads(line)
                task_id = str(record["task_id"])
                self.test_cases[task_id] = record
        
        print(f"Loaded {len(self.test_cases)} test cases from {path}")
    
    def get_test_code(self, task_id: str) -> Optional[str]:
        """Get test code for a task."""
        if task_id not in self.test_cases:
            return None
        return self.test_cases[task_id].get("tests", "")
    
    def get_entry_point(self, task_id: str) -> str:
        """Get entry point function name for a task."""
        if task_id not in self.test_cases:
            return "solution"
        return self.test_cases[task_id].get("entry_point", "solution")
    
    def extract_code(self, response: str) -> str:
        """Extract code from model response (handles markdown blocks)."""
        import re
        
        # Try to find code in markdown blocks
        patterns = [
            r'```python\n(.*?)```',
            r'```\n(.*?)```',
            r'```(.*?)```'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # No markdown block, return as-is
        return response.strip()
    
    def build_test_file(
        self,
        generated_code: str,
        task_id: str,
        dataset_type: str = "humaneval"
    ) -> str:
        """
        Build complete test file combining code and tests.
        
        Args:
            generated_code: Code generated by model
            task_id: Task identifier
            dataset_type: "humaneval" or "mbpp"
            
        Returns:
            Complete Python code ready for pytest
        """
        code = self.extract_code(generated_code)
        test_code = self.get_test_code(task_id)
        entry_point = self.get_entry_point(task_id)
        
        if not test_code:
            raise ValueError(f"No test cases found for task: {task_id}")
        
        if dataset_type == "humaneval":
            # HumanEval format:
            # - prompt includes function signature
            # - test has check(candidate) function
            # - Need to call check(entry_point)
            return self._build_humaneval_test(code, test_code, entry_point)
        else:
            # MBPP format:
            # - tests are assertions that call the function directly
            return self._build_mbpp_test(code, test_code)
    
    def _build_humaneval_test(
        self,
        code: str,
        test_code: str,
        entry_point: str
    ) -> str:
        """Build test file for HumanEval format."""
        # HumanEval tests look like:
        #   def check(candidate):
        #       assert candidate([1,2,3]) == True
        #   check(has_close_elements)
        
        return f'''# Generated solution
{code}

# Test code
{test_code}

# Run check
check({entry_point})
'''
    
    def _build_mbpp_test(self, code: str, test_code: str) -> str:
        """Build test file for MBPP format."""
        # MBPP tests use test_solution() function
        return f'''# Generated solution
{code}

# Tests
{test_code}

# Run tests
test_solution()
'''
    
    def verify(
        self,
        generated_code: str,
        task_id: str,
        dataset_type: str = "humaneval"
    ) -> VerifyResult:
        """
        Verify generated code against test cases.
        
        Args:
            generated_code: Code generated by model
            task_id: Task identifier to look up tests
            dataset_type: "humaneval" or "mbpp"
            
        Returns:
            VerifyResult with success/failure and reward
        """
        try:
            # Build complete test file
            test_file_content = self.build_test_file(
                generated_code, task_id, dataset_type
            )
        except ValueError as e:
            return VerifyResult(
                success=False,
                reward=0.0,
                details=str(e),
                error=str(e)
            )
        
        # Run in temp directory
        with tempfile.TemporaryDirectory() as tmpdir:
            test_file = Path(tmpdir) / "test_solution.py"
            test_file.write_text(test_file_content)
            
            # HumanEval uses check(candidate) - run directly with python
            # MBPP uses assertions - also works with direct python execution
            cmd = ["python", str(test_file)]
            
            try:
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=self.timeout,
                    cwd=tmpdir
                )
                
                success = result.returncode == 0
                
                # For direct execution, success = all assertions passed
                if success:
                    reward = 1.0
                    tests_passed = 1
                    tests_total = 1
                else:
                    reward = 0.0
                    tests_passed = 0
                    tests_total = 1
                
                return VerifyResult(
                    success=success,
                    reward=reward,
                    details=result.stdout[-500:] if result.stdout else "",
                    error=result.stderr[-500:] if not success else None,
                    tests_passed=tests_passed,
                    tests_total=tests_total
                )
                
            except subprocess.TimeoutExpired:
                return VerifyResult(
                    success=False,
                    reward=0.0,
                    details="Test execution timed out",
                    error=f"Timeout after {self.timeout}s"
                )
            except Exception as e:
                return VerifyResult(
                    success=False,
                    reward=0.0,
                    details="Test execution failed",
                    error=str(e)
                )
    
    def _parse_pytest_output(self, output: str) -> tuple:
        """Parse pytest output for pass/fail counts."""
        import re
        
        # Look for "X passed" or "X passed, Y failed"
        match = re.search(r'(\d+) passed', output)
        passed = int(match.group(1)) if match else 0
        
        match = re.search(r'(\d+) failed', output)
        failed = int(match.group(1)) if match else 0
        
        match = re.search(r'(\d+) error', output)
        errors = int(match.group(1)) if match else 0
        
        total = passed + failed + errors
        if total == 0:
            total = 1  # Avoid division by zero
        
        return passed, total
    
    def verify_batch(
        self,
        samples: List[Dict[str, str]],
        dataset_type: str = "humaneval"
    ) -> List[VerifyResult]:
        """
        Verify multiple samples.
        
        Args:
            samples: List of {"code": ..., "task_id": ...}
            dataset_type: "humaneval" or "mbpp"
            
        Returns:
            List of VerifyResult
        """
        results = []
        for sample in samples:
            result = self.verify(
                sample["code"],
                sample["task_id"],
                dataset_type
            )
            results.append(result)
        return results


class HumanEvalVerifier(RLVRPytestVerifier):
    """Convenience class for HumanEval verification."""
    
    def __init__(self, dataset_path: str = "data/rlvr/humaneval_full.jsonl", **kwargs):
        super().__init__(dataset_path, **kwargs)
        self.dataset_type = "humaneval"
    
    def verify(self, generated_code: str, task_id: str) -> VerifyResult:
        return super().verify(generated_code, task_id, self.dataset_type)


class MBPPVerifier(RLVRPytestVerifier):
    """Convenience class for MBPP verification."""
    
    def __init__(self, dataset_path: str = "data/rlvr/mbpp_train_full.jsonl", **kwargs):
        super().__init__(dataset_path, **kwargs)
        self.dataset_type = "mbpp"
    
    def verify(self, generated_code: str, task_id: str) -> VerifyResult:
        return super().verify(generated_code, task_id, self.dataset_type)


# =============================================================================
# Quick test
# =============================================================================

if __name__ == "__main__":
    import sys
    
    # Test with MBPP-style format (test_solution function)
    test_dataset = [
        {
            "task_id": "test/0",
            "prompt": "Write a function add(a, b) that returns a + b",
            "tests": "def test_solution():\n    assert add(1, 2) == 3\n    assert add(0, 0) == 0",
            "entry_point": "add"
        }
    ]
    
    # Write temp dataset
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        for record in test_dataset:
            f.write(json.dumps(record) + '\n')
        temp_path = f.name
    
    try:
        verifier = RLVRPytestVerifier(temp_path)
        
        # Test correct code
        correct_code = "def add(a, b):\n    return a + b"
        result = verifier.verify(correct_code, "test/0", "mbpp")
        print(f"Correct code: success={result.success}, reward={result.reward}")
        assert result.success, "Correct code should pass"
        
        # Test incorrect code
        wrong_code = "def add(a, b):\n    return a - b"
        result = verifier.verify(wrong_code, "test/0", "mbpp")
        print(f"Wrong code: success={result.success}, reward={result.reward}")
        assert not result.success, "Wrong code should fail"
        
        # Test code that doesn't compile
        broken_code = "def add(a, b)\n    return a + b"
        result = verifier.verify(broken_code, "test/0", "mbpp")
        print(f"Broken code: success={result.success}, reward={result.reward}")
        assert not result.success, "Broken code should fail"
        
        print("\nAll self-tests passed!")
        
    finally:
        Path(temp_path).unlink()
